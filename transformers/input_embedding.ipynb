{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T06:28:58.418218500Z",
     "start_time": "2025-04-21T06:28:58.408155600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Token Embedding + Positional Encoding:\n",
    "Token Embedding: 把token转换为vector\n",
    "Positional Encoding:让模型知道词和词的顺序\n",
    "\n",
    "Positional Encoding的作用是给每个token一个在句子中处于第几个的信息\n",
    "`Z = PE(X)`$z_i = x_i + p_i$，其中$x_i$是第i个词的embedding，$p_i$是第i个位置的encoding\n",
    "\n",
    "Sinusoidal Positional Encoding的原理：\n",
    "原文的说法是\n",
    ">we must inject some information about the relative or absolute position of the\n",
    "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "as the embeddings, so that the two can be summed.\n",
    "\n",
    "也就是通过使用多个频率的$sin$叠加，编码出position信息，使得这个PE具有相对、绝对（对于我来说认为是局部或者整体）的position信息。\n",
    "对于位置$pos \\in \\{0,1,2,...\\}$，维度$i \\in \\{0,1,...,d_{model}-1\\}$\n",
    "$\\text{PE}_{pos, 2i} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "\\text{PE}_{pos, 2i+1} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)$\n",
    "\n",
    "最后输出$z_i = x_i + p_i$\n",
    "个人理解：\n",
    "假设d_model = 512:\n",
    "说明对于一个token embedding来说，有一个512维的PE encoding，包含256组(pair) sin cos函数：\n",
    "而这256对sin cos函数，每一对的frequency是不一样的(1pair内的sin cos是一样的frequency)。 -->也就是对encoding进行了傅里叶编码。\n",
    "先说为什么要一对sin cos:\n",
    "sin、cos是orthogonal的，不包含任何重复、冗余的信息\n",
    "再说说为什么要很多对不同的函数：\n",
    "利用多频率正交信号，使得每一对 sin/cos 捕捉一种尺度下的位置信息\n",
    "多尺度并联就构成了“序列的空间坐标系” → 模型可以在这个坐标系中，通过点积、加权感知 token 的相对位置\n",
    "\n",
    "\n",
    "sin(x),cos(x)关于orthogonal的推导：\n",
    "$\n",
    "\\text{定义函数空间中的内积：} \\quad\n",
    "\\langle f, g \\rangle = \\int_a^b f(x) g(x) \\, dx \\\\\n",
    "\n",
    "\\text{设} \\ f(x) = \\sin(x),\\ g(x) = \\cos(x),\\ \\text{区间为} [0, 2\\pi] \\\\\n",
    "\n",
    "\\Rightarrow\n",
    "\\langle \\sin(x), \\cos(x) \\rangle = \\int_0^{2\\pi} \\sin(x) \\cos(x) \\, dx \\\\\n",
    "\n",
    "\\text{利用三角恒等式：} \\quad\n",
    "\\sin(x) \\cos(x) = \\frac{1}{2} \\sin(2x) \\\\\n",
    "\n",
    "\\Rightarrow\n",
    "\\int_0^{2\\pi} \\sin(x) \\cos(x) \\, dx = \\frac{1}{2} \\int_0^{2\\pi} \\sin(2x) \\, dx = 0 \\\\\n",
    "\n",
    "\\therefore \\quad \\sin(x) \\perp \\cos(x)\n",
    "\n",
    "$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caf1e3211511d72c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "    def forward(self,x):\n",
    "        #  embedding(x): (batch_size, seq_len, d_model) --> embedding(x):(vocab_size,d_model)\n",
    "        return self.embedding(x)\n",
    "    \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) #奇数列\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) #偶数列\n",
    "        pe = pe.unsqueeze(0)  # [shape] pe: (1, max_len, d_model)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [shape] x: (batch_size, seq_len, d_model) pe:(1, max_len, d_model) 也就是说只需要提供奇拿\n",
    "        # x.size(1) = seq_len --> pe => (1,seq_len,d_model) x=>(batch_suze,seq_len,d_model)\n",
    "        x = x + self.pe[:, :x.size(1)].to(x.device)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T06:28:58.432248100Z",
     "start_time": "2025-04-21T06:28:58.413217900Z"
    }
   },
   "id": "f6c205242fc379dd",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22., 24., 26.,\n",
      "        28., 30.])\n",
      "tensor([  -0.0000,  -18.4207,  -36.8414,  -55.2620,  -73.6827,  -92.1034,\n",
      "        -110.5241, -128.9448, -147.3654, -165.7861, -184.2068, -202.6275,\n",
      "        -221.0482, -239.4689, -257.8895, -276.3102])\n",
      "tensor([-0.0000, -0.5756, -1.1513, -1.7269, -2.3026, -2.8782, -3.4539, -4.0295,\n",
      "        -4.6052, -5.1808, -5.7565, -6.3321, -6.9078, -7.4834, -8.0590, -8.6347])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(0, 32, 2).float())\n",
    "print(torch.arange(0, 32, 2).float() * (-math.log(10000.0)))\n",
    "print(torch.arange(0, 32, 2).float() * (-math.log(10000.0)/32)) # 原论文中的encoding函数"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T06:28:58.443819Z",
     "start_time": "2025-04-21T06:28:58.423247800Z"
    }
   },
   "id": "cbb0a5e2c712f7a6",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512])\n",
      "torch.Size([1, 3, 512])\n",
      "torch.Size([1, 5000, 3])\n"
     ]
    }
   ],
   "source": [
    "pe_test = torch.zeros((1,5000,512))\n",
    "print(pe_test[:,:3].shape)\n",
    "print(pe_test[:,:3,:].shape)\n",
    "print(pe_test[:,:,:3].shape)\n",
    "del pe_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T06:28:58.458698300Z",
     "start_time": "2025-04-21T06:28:58.435314500Z"
    }
   },
   "id": "e52c9a3cc9bbe3c9",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [shape] x: (batch_size, seq_len)\n",
    "        x = self.token_embedding(x)  # TokenEmbedding@x.shape → (batch, seq_len, d_model)\n",
    "        x = self.pos_encoding(x)     # PositionalEncoding@x.shape → (batch, seq_len, d_model)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T06:28:58.459698500Z",
     "start_time": "2025-04-21T06:28:58.446812100Z"
    }
   },
   "id": "6e269ca5b88004cc",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9c71a0c1db84bb38"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([2, 10])\n",
      "Output embedding shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_input_embedding():\n",
    "    vocab_size = 1000\n",
    "    d_model = 512\n",
    "    seq_len = 10\n",
    "    batch_size = 2\n",
    "\n",
    "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    embedder = InputEmbedding(vocab_size, d_model, max_len=seq_len)\n",
    "\n",
    "    out = embedder(input_ids)\n",
    "    print(\"Input IDs shape:\", input_ids.shape)         # (2, 10)\n",
    "    print(\"Output embedding shape:\", out.shape)        # (2, 10, 512)\n",
    "test_input_embedding()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T06:28:58.472037900Z",
     "start_time": "2025-04-21T06:28:58.455698500Z"
    }
   },
   "id": "c49c39987841211b",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T06:28:58.494038300Z",
     "start_time": "2025-04-21T06:28:58.468191400Z"
    }
   },
   "id": "b88f1c51f71540c3",
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
