{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Decoder \n",
    "一个Decoder Block包含三层结构：\n",
    "Masked Self-Attention\n",
    "Cross-Attention\n",
    "Feed-Forward Network\n",
    "其中为了防止过拟合，参数爆炸等问题，都加入LayerNorm Dropout Residual Connection\n",
    "```\n",
    "          Input (x)      ← shape: (batch, tgt_len, d_model)\n",
    "               │\n",
    "        ┌──────▼──────┐\n",
    "        │  LayerNorm  │\n",
    "        └──────┬──────┘\n",
    "               ↓\n",
    "     Masked Multi-Head Self-Attention\n",
    "               ↓\n",
    "          Dropout + Residual (+ x)\n",
    "               │\n",
    "        ┌──────▼──────┐\n",
    "        │  LayerNorm  │\n",
    "        └──────┬──────┘\n",
    "               ↓\n",
    "   Multi-Head Cross-Attention (with encoder output)\n",
    "               ↓\n",
    "          Dropout + Residual\n",
    "               │\n",
    "        ┌──────▼──────┐\n",
    "        │  LayerNorm  │\n",
    "        └──────┬──────┘\n",
    "               ↓\n",
    "       FeedForward Network (2-layer MLP)\n",
    "               ↓\n",
    "          Dropout + Residual\n",
    "               │\n",
    "               ▼\n",
    "        Output (same shape as input)\n",
    "\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74486a2c8242f05a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.MultiHeadAttentijon import MultiHeadAttention"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-01T02:22:45.929266700Z",
     "start_time": "2025-05-01T02:22:45.883752500Z"
    }
   },
   "id": "initial_id",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, tgt_padding_mask=None, memory_padding_mask=None):\n",
    "        \"\"\"\n",
    "        :param x: decoder input (batch_size, tgt_len, d_model)\n",
    "        :param encoder_output: encoder output (batch_size, src_len, d_model)\n",
    "        :param tgt_padding_mask: (batch_size, tgt_len)\n",
    "        :param memory_padding_mask: (batch_size, src_len)\n",
    "        \"\"\"\n",
    "        #  Masked Self-Attention (Q=K=V=x)\n",
    "        x_norm = self.norm1(x)\n",
    "        self_attn_out, _ = self.self_attn(\n",
    "            x_norm,\n",
    "            query=x_norm,\n",
    "            key=x_norm,\n",
    "            value=x_norm,\n",
    "            padding_mask=tgt_padding_mask,\n",
    "            causal_mask=True\n",
    "        )\n",
    "        x = x + self.dropout1(self_attn_out)\n",
    "\n",
    "        # Cross Attention (Q = decoder, K/V = encoder)\n",
    "        x_norm = self.norm2(x)\n",
    "        cross_attn_out, _ = self.cross_attn(\n",
    "            x_norm,\n",
    "            query=x_norm,\n",
    "            key=encoder_output,\n",
    "            value=encoder_output,\n",
    "            padding_mask=memory_padding_mask,\n",
    "            causal_mask=False\n",
    "        )\n",
    "        x = x + self.dropout2(cross_attn_out)\n",
    "\n",
    "        #  Feed Forward Network\n",
    "        x_norm = self.norm3(x)\n",
    "        ffn_out = self.ffn(x_norm)\n",
    "        x = x + self.dropout3(ffn_out)\n",
    "\n",
    "        return x  # shape: (batch_size, tgt_len, d_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T02:22:45.932268700Z",
     "start_time": "2025-05-01T02:22:45.918270100Z"
    }
   },
   "id": "ad695fd396f8672b",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer Decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ad9f1a6b408ac5e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input shape: torch.Size([2, 6, 512])\n",
      "Encoder output shape: torch.Size([2, 8, 512])\n",
      "Decoder output shape: torch.Size([2, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_decoder_block():\n",
    "    batch_size = 2\n",
    "    tgt_len = 6     # decoder 输入长度（如生成的 token 数）\n",
    "    src_len = 8     # encoder 输出长度（源句子 token 数）\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    dim_ff = 2048\n",
    "\n",
    "    # 输入数据：decoder input + encoder output\n",
    "    decoder_input = torch.randn(batch_size, tgt_len, d_model)\n",
    "    encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "\n",
    "    # padding mask：1 表示 padding，需要被屏蔽\n",
    "    tgt_padding_mask = torch.tensor([\n",
    "        [0, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "    ])  # (batch_size, tgt_len)\n",
    "\n",
    "    memory_padding_mask = torch.tensor([\n",
    "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    ])  # (batch_size, src_len)\n",
    "\n",
    "    # 初始化 DecoderBlock\n",
    "    decoder_block = TransformerDecoderBlock(\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dim_ff=dim_ff,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    output = decoder_block(\n",
    "        decoder_input,\n",
    "        encoder_output,\n",
    "        tgt_padding_mask=tgt_padding_mask,\n",
    "        memory_padding_mask=memory_padding_mask\n",
    "    )\n",
    "\n",
    "    print(\"Decoder input shape:\", decoder_input.shape)         # (2, 6, 512)\n",
    "    print(\"Encoder output shape:\", encoder_output.shape)       # (2, 8, 512)\n",
    "    print(\"Decoder output shape:\", output.shape)               # (2, 6, 512)\n",
    "test_decoder_block()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T02:22:45.963280200Z",
     "start_time": "2025-05-01T02:22:45.936269700Z"
    }
   },
   "id": "850d8eb1887e318d",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(d_model, num_heads, dim_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)  # 最后加个输出归一化（论文原版也有）\n",
    "\n",
    "    def forward(self, x, encoder_output, tgt_padding_mask=None, memory_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                encoder_output,\n",
    "                tgt_padding_mask=tgt_padding_mask,\n",
    "                memory_padding_mask=memory_padding_mask\n",
    "            )\n",
    "        return self.norm(x)  # (batch_size, tgt_len, d_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T02:22:45.967278600Z",
     "start_time": "2025-05-01T02:22:45.956278900Z"
    }
   },
   "id": "5976be290d147549",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input shape:    torch.Size([2, 6, 512])\n",
      "Encoder output shape:   torch.Size([2, 8, 512])\n",
      "Decoder output shape:   torch.Size([2, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_transformer_decoder():\n",
    "    batch_size = 2\n",
    "    tgt_len = 6     # decoder 输入长度（目标序列）\n",
    "    src_len = 8     # encoder 输出长度（源序列）\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    dim_ff = 2048\n",
    "    num_layers = 4\n",
    "\n",
    "    decoder_input = torch.randn(batch_size, tgt_len, d_model)\n",
    "    encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "\n",
    "    tgt_padding_mask = torch.tensor([\n",
    "        [0, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "    ])  # shape: (2, 6)\n",
    "\n",
    "    memory_padding_mask = torch.tensor([\n",
    "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    ])  # shape: (2, 8)\n",
    "\n",
    "    decoder = TransformerDecoder(\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dim_ff=dim_ff,\n",
    "        num_layers=num_layers,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    output = decoder(\n",
    "        decoder_input,\n",
    "        encoder_output,\n",
    "        tgt_padding_mask=tgt_padding_mask,\n",
    "        memory_padding_mask=memory_padding_mask\n",
    "    )\n",
    "\n",
    "    print(\"Decoder input shape:   \", decoder_input.shape)    # (2, 6, 512)\n",
    "    print(\"Encoder output shape:  \", encoder_output.shape)   # (2, 8, 512)\n",
    "    print(\"Decoder output shape:  \", output.shape)           # (2, 6, 512)\n",
    "test_transformer_decoder()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T02:22:55.516127600Z",
     "start_time": "2025-05-01T02:22:55.442389300Z"
    }
   },
   "id": "77e80d7bb63ed83c",
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
