{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-29T20:03:48.568188100Z",
     "start_time": "2025-04-29T20:03:45.151821200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer Encoder Block结构是：\n",
    "```methamatica\n",
    "Input → MultiHeadSelfAttention → Add & Norm\n",
    "      → FeedForward → Add & Norm\n",
    "```\n",
    "\n",
    "原版实现使用的post-LN，但是有梯度消失、爆炸的隐患\n",
    "下面实现Pre-LN的结构，也就是LayerNorm放在其他子层输入之前"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "136e4a2f731adaa8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers.MultiHeadAttentijon import MultiHeadAttention\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),   # Linear@x.shape=(batch_size, seq_len, d_model) → (batch_size, seq_len, dim_ff)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_ff, d_model)    # Linear@x.shape=(batch_size, seq_len, dim_ff) → (batch_size, seq_len, d_model)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        # [size] x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Pre-LN + Attention + Dropout + Residual\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(\n",
    "            x_norm,\n",
    "            padding_mask=padding_mask,\n",
    "            causal_mask=False\n",
    "        )\n",
    "        x = x + self.dropout1(attn_out)\n",
    "\n",
    "        # Pre-LN + FFN + Dropout + Residual\n",
    "        x_ffn = self.norm2(x)\n",
    "        ffn_out = self.ffn(x_ffn)\n",
    "        x = x + self.dropout2(ffn_out)\n",
    "\n",
    "        return x  # (batch_size, seq_len, d_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-29T20:03:48.587918Z",
     "start_time": "2025-04-29T20:03:48.571190400Z"
    }
   },
   "id": "70535cec28605352",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "结构长这样\n",
    "```\n",
    "      Input (x)\n",
    "         │\n",
    "    ┌────┴────┐\n",
    "    ↓         ↓\n",
    " Self-Attn   Shortcut\n",
    "    ↓         ↓\n",
    " Dropout     │\n",
    "    ↓        │\n",
    "    Add ◄────┘\n",
    "     ↓  ────────       \n",
    " LayerNorm      │\n",
    "     ↓          │\n",
    " FeedForward    │\n",
    "     ↓          │\n",
    " Dropout        │   \n",
    "     ↓          │\n",
    " Add ◄──────────  \n",
    "     ↓     \n",
    " LayerNorm \n",
    "      ↓    \n",
    "   Output  \n",
    "\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ccb73467ca04004"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 6, 512])\n",
      "Output shape: torch.Size([2, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_encoder_block_pre_ln():\n",
    "    batch_size = 2\n",
    "    seq_len = 6\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    dim_ff = 2048\n",
    "\n",
    "    # 模拟输入\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # 模拟 padding mask（batch 中第 2 个序列有 3 个 padding）\n",
    "    padding_mask = torch.tensor([\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1]\n",
    "    ])  # shape: (batch_size, seq_len)\n",
    "\n",
    "    # 创建模块并测试\n",
    "    block = TransformerEncoderBlock(d_model=d_model, num_heads=num_heads, dim_ff=dim_ff)\n",
    "    out = block(x, padding_mask=padding_mask)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)       # (2, 6, 512)\n",
    "    print(\"Output shape:\", out.shape)    # (2, 6, 512)\n",
    "\n",
    "test_encoder_block_pre_ln()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-29T20:03:48.680986700Z",
     "start_time": "2025-04-29T20:03:48.576918100Z"
    }
   },
   "id": "22c3ce177362e4d",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, num_heads, dim_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, padding_mask=padding_mask)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-29T20:03:48.680986700Z",
     "start_time": "2025-04-29T20:03:48.656492300Z"
    }
   },
   "id": "34f2ed5759ccef09",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 6, 512])\n",
      "Output shape: torch.Size([2, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "def test_transformer_encoder():\n",
    "    batch_size = 2\n",
    "    seq_len = 6\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    dim_ff = 2048\n",
    "    num_layers = 3\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    padding_mask = torch.tensor([\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1]\n",
    "    ])\n",
    "\n",
    "    encoder = TransformerEncoder(d_model, num_heads, dim_ff, num_layers)\n",
    "    out = encoder(x, padding_mask=padding_mask)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", out.shape)\n",
    "test_transformer_encoder()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-29T20:03:48.726843900Z",
     "start_time": "2025-04-29T20:03:48.659984700Z"
    }
   },
   "id": "7089847cdd30106",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "和input embedding层混合在一起，形成一个完整的encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc431b37f723f829"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from InputEmbedding import InputEmbedding\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, dim_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_embedding = InputEmbedding(vocab_size, d_model, max_len)\n",
    "        self.encoder = TransformerEncoder(d_model, num_heads, dim_ff, num_layers, dropout)\n",
    "\n",
    "    def forward(self, input_ids, padding_mask=None):\n",
    "        # [size] input_ids: (batch_size, seq_len)\n",
    "        x = self.input_embedding(input_ids)  # (batch_size, seq_len, d_model)\n",
    "        x = self.encoder(x, padding_mask=padding_mask)  # (batch_size, seq_len, d_model)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-29T20:10:55.697265200Z",
     "start_time": "2025-04-29T20:10:55.693265900Z"
    }
   },
   "id": "9875a772c635adb5",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([2, 10])\n",
      "Encoder output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    vocab_size = 1000\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    dim_ff = 2048\n",
    "    num_layers = 6\n",
    "    seq_len = 10\n",
    "    batch_size = 2\n",
    "\n",
    "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    padding_mask = torch.tensor([\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],    # No padding\n",
    "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]     # Padding after 4th token\n",
    "    ])\n",
    "\n",
    "    model = TransformerEncoderModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dim_ff=dim_ff,\n",
    "        num_layers=num_layers,\n",
    "        max_len=5000,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    out = model(input_ids, padding_mask=padding_mask)\n",
    "    print(\"Input IDs shape:\", input_ids.shape)   # (2, 10)\n",
    "    print(\"Encoder output shape:\", out.shape)     # (2, 10, 512)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-29T20:11:01.652362400Z",
     "start_time": "2025-04-29T20:11:01.544603900Z"
    }
   },
   "id": "ece11fee2470d6b2",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f339e4d5dd7a8b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
