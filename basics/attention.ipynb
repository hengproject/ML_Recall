{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03c4f96b2de1aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "credit to: [笔记：hwcoder](https://hwcoder.top/Manual-Coding-1) and [知乎：论文精读之Recurrent Models of Visual Attention](https://zhuanlan.zhihu.com/p/555778029)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Additive Attention\n",
    "2014 年[《Neural Machine Translation by Jointly Learning to Align and Translate》](https://arxiv.org/abs/1409.0473)\n",
    "这虽然不是最早的attention机制提出者，但是提出了注意力机制在机器翻译中的应用。\n",
    "核心思想是：在生成每一个目标词时，自动对源句子的所有词分配权重（注意力），然后根据这些权重加权求和出一个“上下文向量”，作为当前步的辅助信息。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa8f8de7b02f024e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "deb2fa1b16d821c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "对于decoder每个时间步$i$, 有$s_{i-1}$:是上一步的hidden state (原论文的公式1)，和$\\{h_1,...,h_T\\}$:encoder所有时间步的hidden state(encoder输入了T个token的时候)\n",
    "那么生成目标词$y_i$就需要关注这个词最应该关注原来语句中的哪些token\n",
    "\n",
    "（原论文第七页A.2.2 decoder architecture)\n",
    "第一步：计算匹配程度\n",
    "对于decoder的第i步和encoder的第j步\n",
    "计算decoder第i步的状态($s_{i-1}$)与 encoder第j个字符之间的关系($h_j$)\n",
    "$e_{ij} = v_a^\\top \\tanh(W_a s_{i-1} + U_a h_j)$ 其中的$v_a^\\top \\in \\mathbb{R}^{n'} \\ \\ \\ \\ W_a \\in \\mathbb{R}^{n' \\times n} \\ \\ \\ U_a \\in \\mathbb{R}^{n' \\times 2n} $是可训练参数，$tanh$ 是激活函数。 以鄙人之见，这玩意就相当于一个一层MLP，作为Universal Approximator的，用来估计$s_{i-1}$和$h_j$的相似度的，而$s_{i-1}$代表着当前词汇的当前语境。\n",
    "\n",
    "第二步：softmax归一化注意力权重 $\\alpha_{ij}$\n",
    "对于每个encoder位置j,计算一个概率权重($\\alpha_{ij}$,$\\sum \\alpha_{ij} = 1$):\n",
    "$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}$\n",
    "就是一个softmax函数，用来1.exp放大差距2.将$e_{ij}$归一化并转换成非负权重\n",
    "\n",
    "第三步：用注意力权重计算上下文向量$c_i$\n",
    "$c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j$\n",
    "\n",
    "$c_i$表示的是一个上下文变量，表示decoder第i个字符对于encoder中每个字符的相关度(attention值)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3acec9c23bf12bc8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2025)"
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-04-19T08:05:47.480671800Z",
     "start_time": "2025-04-19T08:05:47.222663500Z"
    }
   },
   "id": "initial_id",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5,) (5, 4)\n",
      "Context vector (c_i): [-0.07463606  0.69465917  0.25594682 -0.61618757]\n",
      "Attention weights (α): [0.16482456 0.15648484 0.1736355  0.34484006 0.16021504]\n",
      "Sum of attention weights: 1.0\n"
     ]
    }
   ],
   "source": [
    "def additive_attention(s_prev,h,Wa,Ua,va):\n",
    "    \"\"\"\n",
    "    :param s_prev: (n,)上一步decoder的hidden state (s_{j-1})\n",
    "    :param h: (Tx,n)所有encoder的hidden state\n",
    "    :param Wa: (d,n)s_prev的 projection matrix \n",
    "    :param Ua: (d,n)h的projection matrix\n",
    "    :param va: (d,)\n",
    "    :return: context:(n,)context vector c_i alpha:(Tx,)注意力权重 (Tx是输入长度,即encoder长度)\n",
    "    \"\"\"\n",
    "    Tx,n = h.shape\n",
    "    d = va.shape\n",
    "    s_expanded = np.tile(s_prev,(Tx,1))\n",
    "    \n",
    "    e = np.tanh(s_expanded@Wa.T + h@Ua.T)@va\n",
    "    # alpha_ij = softmax(e_ij)\n",
    "    print(e.shape)\n",
    "    alpha = np.exp(e - np.max(e))  # 避免数值不稳定\n",
    "    alpha = alpha / np.sum(alpha)  # (Tx,)\n",
    "    # print(alpha.shape) (Tx,n)\n",
    "    # context vector c_i = sum_j alpha_ij * h_j\n",
    "    print(alpha.shape,h.shape)# 输出(5,) (5, 4)\n",
    "    # context = np.sum(alpha[:,np.newaxis] * h, axis=0)  # np.sum((Tx, n), axis=0) → (n,)\n",
    "    context = alpha@h #(Tx,)@(Tx,n) = (n,)\n",
    "    return context, alpha\n",
    "\n",
    "\n",
    "def test_additive_attention():\n",
    "    Tx = 5 # length of encoder\n",
    "    n = 4 # hidden_size(defined in RNN/DRU/LSTM)\n",
    "    d = 3 # attention dimension\n",
    "    \n",
    "    s_prev = np.random.randn(n)            # decoder state (n,)\n",
    "    h = np.random.randn(Tx, n)             # encoder hidden states (Tx, n)\n",
    "    Wa = np.random.randn(d, n)              \n",
    "    Ua = np.random.randn(d, n)\n",
    "    va = np.random.randn(d)\n",
    "    context, alpha = additive_attention(s_prev, h, Wa, Ua, va)\n",
    "\n",
    "    print(\"Context vector (c_i):\", context)\n",
    "    print(\"Attention weights (α):\", alpha)\n",
    "    print(\"Sum of attention weights:\", np.sum(alpha))  # 应该接近 1\n",
    "    \n",
    "test_additive_attention()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-19T08:49:43.215728700Z",
     "start_time": "2025-04-19T08:49:43.206745600Z"
    }
   },
   "id": "279d3f27b2c6709a",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "再附赠一个pytorch版本的\n",
    "这里$W_a$和$U_a$用了linear实现，支持batch操作\n",
    "$$\n",
    "s_{exp} \\in \\mathbb{R}^{batch \\times T_x \\times hidden} \\quad \\@ \\quad W_a \\in \\mathbb{R}^{hidden \\times attn}\\quad \\Rightarrow \\quad \\mathbb{R}^{batch \\times T_x \\times attn}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f66a6e7e592cfefa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_prev shape: torch.Size([2, 4])\n",
      "h shape: torch.Size([2, 5, 4])\n",
      "context shape: torch.Size([2, 4])\n",
      "alpha shape: torch.Size([2, 5])\n",
      "alpha sum (per batch): tensor([1., 1.], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_dim):\n",
    "        super().__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, attention_dim, bias=False)\n",
    "        self.Ua = nn.Linear(hidden_size, attention_dim, bias=False)\n",
    "        self.va = nn.Parameter(torch.randn(attention_dim))  # learnable vector\n",
    "\n",
    "    def forward(self, s_prev:torch.Tensor, h):\n",
    "        \"\"\"\n",
    "        :param s_prev: (batch, hidden_size) decoder 上一步 hidden state\n",
    "        :param h: (batch, Tx, hidden_size) 所有 encoder hidden state\n",
    "        :return: context: (batch, hidden_size), alpha: (batch, Tx)\n",
    "        \"\"\"\n",
    "        # 1. expand s_prev → (batch, Tx, hidden_size)\n",
    "        batch_size, Tx, _ = h.size()\n",
    "        s_exp = s_prev.unsqueeze(1).expand(-1, Tx, -1)\n",
    "        # s_prev (batch, hidden_size) -> unsqueeze(1) (batch, 1, hidden_size) -> expand (batch, Tx, hidden_size)\n",
    "        # 所以s_exp =>(batch, Tx, hidden_size)\n",
    "        # 2. 计算注意力 energy scores\n",
    "        e = torch.tanh(self.Wa(s_exp) + self.Ua(h))  # (batch, Tx, attn_dim)\n",
    "        e_scores = torch.matmul(e, self.va)  # (batch, Tx) (batch, Tx, attn_dim) @ (attn_dim,) → (batch, Tx)\n",
    "\n",
    "        # 3. softmax 得到注意力分布\n",
    "        alpha = F.softmax(e_scores, dim=1)  # (batch, Tx)\n",
    "\n",
    "        # 4. 上下文向量 c_i = alpha @ h\n",
    "        context = torch.bmm(alpha.unsqueeze(1), h)  # torch.bmm((batch, 1, Tx), (batch, Tx, hidden)) → (batch, 1, hidden)\n",
    "        context = context.squeeze(1)  # (batch, hidden_size)\n",
    "\n",
    "        return context, alpha\n",
    "def test_torch_additive_attention():\n",
    "    batch_size = 2\n",
    "    Tx = 5              # encoder 时间步\n",
    "    hidden_size = 4     # encoder/decoder hidden dim\n",
    "    attention_dim = 3   # attention 隐空间维度\n",
    "\n",
    "    # 设置设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 随机输入：decoder state + encoder states\n",
    "    s_prev = torch.randn(batch_size, hidden_size).to(device)       # (batch, hidden_size)\n",
    "    h = torch.randn(batch_size, Tx, hidden_size).to(device)        # (batch, Tx, hidden_size)\n",
    "\n",
    "    # 初始化 attention 模块\n",
    "    attn = AdditiveAttention(hidden_size, attention_dim).to(device)\n",
    "\n",
    "    # 前向计算\n",
    "    context, alpha = attn(s_prev, h)\n",
    "\n",
    "    # 输出检查\n",
    "    print(\"s_prev shape:\", s_prev.shape)\n",
    "    print(\"h shape:\", h.shape)\n",
    "    print(\"context shape:\", context.shape)  # (batch, hidden_size)\n",
    "    print(\"alpha shape:\", alpha.shape)      # (batch, Tx)\n",
    "    print(\"alpha sum (per batch):\", alpha.sum(dim=1))  # 应该接近 1\n",
    "test_torch_additive_attention()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-19T08:55:32.719685Z",
     "start_time": "2025-04-19T08:55:32.195170500Z"
    }
   },
   "id": "96b5b04f8df91a2f",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%reset -f"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-19T08:57:40.377411100Z",
     "start_time": "2025-04-19T08:57:40.297194400Z"
    }
   },
   "id": "28e4776b3f151bfe",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Transformer: MHA Multi Head Attention\n",
    "2017 年开山之作[《Attention is all you need》](https://arxiv.org/abs/1706.03762)所提出的一种 Attention 形式，可以说它是当前主流 LLM 的基础工作。每个头有自己单独的 Query、Key 和 Value 矩阵。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34ff8027a1d25299"
  },
  {
   "cell_type": "markdown",
   "source": [
    "第一步： 为输入序列中计算 $Q K V$,这是通过将input tokens($X$)和project matrix( $W_q W_k W_v$ )相乘实现的\n",
    "    个人认为：其中Q K是负责计算attention机制的，V是负责输出部分的\n",
    "$\\begin{align*}\n",
    "Q &= X W_q \\\\\n",
    "K &= X W_k \\\\\n",
    "V &= X W_v\n",
    "\\text{其中：} \\\\\n",
    "X &\\in \\mathbb{R}^{n \\times d_{\\text{model}}} \\quad \\text{（输入序列矩阵）} \\\\\n",
    "W_q, W_k, W_v &\\in \\mathbb{R}^{d_{\\text{model}} \\times d_k} \\quad \\text{（投影矩阵）} \\\\\n",
    "Q, K, V &\\in \\mathbb{R}^{n \\times d_k} \\quad \\text{（查询、键、值向量矩阵）}\n",
    "\\end{align*}\n",
    "$\n",
    "$n$ = input sequence length\n",
    "可以注意到参数中维度的$d_{model}$和$d_k$:\n",
    "$d_{model}$ ->这个值指的是什么：1.从embedding角度，每一个token会被转换成一个$d_{model}$ 大小的vector 2.从计算角度，$d_{model}$不止决定了attention的结构，也和网络整体的结构参数数量有关。\n",
    "定义$h$为注意力头数量，$d_k$为每个注意力头中的Query/Key向量的维度,$d_v$为Value的向量的维度,一般有：\n",
    "$d_k = d_v = \\frac{d_{model}}{h}$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1cd2bbade8ce513"
  },
  {
   "cell_type": "markdown",
   "source": [
    "如何从AA迁移到MHA:  $ q = W_q s_{i-1} \\quad k_j = W_k h_j \\quad v_j = W_v h_j$\n",
    "这样看QKV就很类似了"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49340768ab69e09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "第二步： 计算attention得分(Score)并通过softmax得到注意力分布:\n",
    "和之前的additive attention机制类似，不过之前是使用MLP+tanh的方式来计算attention，再MHA中使用Scaled Dot Product来表示\n",
    "Scaled Dot Product公式如下：\n",
    "$\\text{score}(Q, K) = \\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "其中$d_k$是key向量的维度(通常等于每个head的维度)\n",
    "这里解释一下为什么要除以$\\sqrt{d_k}$:\n",
    "已知（或者说假设）$q,k \\in \\mathbb{R}^{d_k}$每个维度都是随机初始化的，也就是说服从Standard Normal Distribution。\n",
    "换句话说，$q, k \\in \\mathbb{R}^{d_k},\\quad q_i, k_i \\sim \\mathcal{N}(0, 1),\\quad \\text{i.i.d.}$\n",
    "那么对于$q \\cot k$，\n",
    "期望就应该是$\\mathbb{E}[q \\cdot k] = \\sum_{i=1}^{d_k} \\mathbb{E}[q_i k_i] = \\sum_{i=1}^{d_k} \\mathbb{E}[q_i] \\mathbb{E}[k_i] = 0$\n",
    "方差就应该是\n",
    "\n",
    "$\\begin{align*}\n",
    "\\text{Var}(q_i k_i) &= \\mathbb{E}[(q_i k_i)^2] - \\mathbb{E}[q_i k_i]^2 \\\\\n",
    "&= \\mathbb{E}[q_i^2] \\cdot \\mathbb{E}[k_i^2] - 0 \\\\\n",
    "&= 1 \\cdot 1 = 1 \\\\\n",
    "\\text{Var}[q \\cdot k] &= \\sum_{i=1}^{d_k} \\text{Var}(q_i k_i) = \\sum_{i=1}^{d_k} 1 = d_k\n",
    "\\end{align*}$\n",
    "\n",
    "这就意味着attention计算出的score值会发生分布的变化,如果除以标准差$\\sqrt{d_k}$就可以保持attention点积分布保持数值稳定\n",
    "如果不进行这一步处理，就会让softmax接近one-hot,导致反向传播机制只关注其中一条路径，难以学习\n",
    "\n",
    "\n",
    "然后再像之前AA中一样，用softmax来归一化\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V\n",
    "$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7d703b3b4561401"
  },
  {
   "cell_type": "markdown",
   "source": [
    "第三步：使用注意力权重加权 Value，得到上下文向量（context vector）\n",
    "在AA中，我们使用了$c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j$的方法来计算context，其中的$\\alpha$是attention score,$h_j$是encoder第j个token\n",
    "所以在transformer中，使用Value来替代$h_j$，更好的获取局部特征\n",
    "$\\text{Attention}(Q, K, V) = \\underbrace{\\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)}_{\\text{score}} \\cdot V\n",
    "$\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78449147e7adf156"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e15c549930f5949b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1889660c66e2248c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "最后，再让我们确认一下他们的维度：\n",
    "$\n",
    "X \\in \\mathbb{R}^{n \\times d_{\\text{model}}} \\\\\n",
    "Q = X W_q,\\quad K = X W_k,\\quad V = X W_v \\\\\n",
    "Q, K, V \\in \\mathbb{R}^{n \\times d_k} \\\\\n",
    "\\text{score} = \\frac{Q K^\\top}{\\sqrt{d_k}} \\in \\mathbb{R}^{n \\times n}\\\\\n",
    "\\alpha = \\text{softmax} \\left( \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) \\in \\mathbb{R}^{n \\times n}\\\\\n",
    "\\text{Output} = \\alpha V \\in \\mathbb{R}^{n \\times d_v}\\\\\n",
    "\\text{Concat}(\\text{head}_1, ..., \\text{head}_h) \\in \\mathbb{R}^{n \\times (h \\cdot d_v)}\\\\\n",
    "\\text{MultiHeadOutput} = \\text{Concat}(\\cdots) W^O \\in \\mathbb{R}^{n \\times d_{\\text{model}}}\\\\\n",
    "\n",
    "$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "236c343170fe34b8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "61991158b822d030"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
