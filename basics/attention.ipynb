{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03c4f96b2de1aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "credit to: [笔记：hwcoder](https://hwcoder.top/Manual-Coding-1) and [知乎：论文精读之Recurrent Models of Visual Attention](https://zhuanlan.zhihu.com/p/555778029)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Additive Attention\n",
    "2014 年[《Neural Machine Translation by Jointly Learning to Align and Translate》](https://arxiv.org/abs/1409.0473)\n",
    "这虽然不是最早的attention机制提出者，但是提出了注意力机制在机器翻译中的应用。\n",
    "核心思想是：在生成每一个目标词时，自动对源句子的所有词分配权重（注意力），然后根据这些权重加权求和出一个“上下文向量”，作为当前步的辅助信息。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa8f8de7b02f024e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "deb2fa1b16d821c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "对于decoder每个时间步$i$, 有$s_{i-1}$:是上一步的hidden state (原论文的公式1)，和$\\{h_1,...,h_T\\}$:encoder所有时间步的hidden state(encoder输入了T个token的时候)\n",
    "那么生成目标词$y_i$就需要关注这个词最应该关注原来语句中的哪些token\n",
    "\n",
    "（原论文第七页A.2.2 decoder architecture)\n",
    "第一步：计算匹配程度\n",
    "对于decoder的第i步和encoder的第j步\n",
    "计算decoder第i步的状态($s_{i-1}$)与 encoder第j个字符之间的关系($h_j$)\n",
    "$e_{ij} = v_a^\\top \\tanh(W_a s_{i-1} + U_a h_j)$ 其中的$v_a^\\top \\in \\mathbb{R}^{n'} \\ \\ \\ \\ W_a \\in \\mathbb{R}^{n' \\times n} \\ \\ \\ U_a \\in \\mathbb{R}^{n' \\times 2n} $是可训练参数，$tanh$ 是激活函数。 以鄙人之见，这玩意就相当于一个一层MLP，作为Universal Approximator的，用来估计$s_{i-1}$和$h_j$的相似度的，而$s_{i-1}$代表着当前词汇的当前语境。\n",
    "\n",
    "第二步：softmax归一化注意力权重 $\\alpha_{ij}$\n",
    "对于每个encoder位置j,计算一个概率权重($\\alpha_{ij}$,$\\sum \\alpha_{ij} = 1$):\n",
    "$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}$\n",
    "就是一个softmax函数，用来1.exp放大差距2.将$e_{ij}$归一化并转换成非负权重\n",
    "\n",
    "第三步：用注意力权重计算上下文向量$c_i$\n",
    "$c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j$\n",
    "\n",
    "$c_i$表示的是一个上下文变量，表示decoder第i个字符对于encoder中每个字符的相关度(attention值)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3acec9c23bf12bc8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2025)"
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-04-19T08:05:47.480671800Z",
     "start_time": "2025-04-19T08:05:47.222663500Z"
    }
   },
   "id": "initial_id",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5,) (5, 4)\n",
      "Context vector (c_i): [-0.07463606  0.69465917  0.25594682 -0.61618757]\n",
      "Attention weights (α): [0.16482456 0.15648484 0.1736355  0.34484006 0.16021504]\n",
      "Sum of attention weights: 1.0\n"
     ]
    }
   ],
   "source": [
    "def additive_attention(s_prev,h,Wa,Ua,va):\n",
    "    \"\"\"\n",
    "    :param s_prev: (n,)上一步decoder的hidden state (s_{j-1})\n",
    "    :param h: (Tx,n)所有encoder的hidden state\n",
    "    :param Wa: (d,n)s_prev的 projection matrix \n",
    "    :param Ua: (d,n)h的projection matrix\n",
    "    :param va: (d,)\n",
    "    :return: context:(n,)context vector c_i alpha:(Tx,)注意力权重 (Tx是输入长度,即encoder长度)\n",
    "    \"\"\"\n",
    "    Tx,n = h.shape\n",
    "    d = va.shape\n",
    "    s_expanded = np.tile(s_prev,(Tx,1))\n",
    "    \n",
    "    e = np.tanh(s_expanded@Wa.T + h@Ua.T)@va\n",
    "    # alpha_ij = softmax(e_ij)\n",
    "    print(e.shape)\n",
    "    alpha = np.exp(e - np.max(e))  # 避免数值不稳定\n",
    "    alpha = alpha / np.sum(alpha)  # (Tx,)\n",
    "    # print(alpha.shape) (Tx,n)\n",
    "    # context vector c_i = sum_j alpha_ij * h_j\n",
    "    print(alpha.shape,h.shape)# 输出(5,) (5, 4)\n",
    "    # context = np.sum(alpha[:,np.newaxis] * h, axis=0)  # np.sum((Tx, n), axis=0) → (n,)\n",
    "    context = alpha@h #(Tx,)@(Tx,n) = (n,)\n",
    "    return context, alpha\n",
    "\n",
    "\n",
    "def test_additive_attention():\n",
    "    Tx = 5 # length of encoder\n",
    "    n = 4 # hidden_size(defined in RNN/DRU/LSTM)\n",
    "    d = 3 # attention dimension\n",
    "    \n",
    "    s_prev = np.random.randn(n)            # decoder state (n,)\n",
    "    h = np.random.randn(Tx, n)             # encoder hidden states (Tx, n)\n",
    "    Wa = np.random.randn(d, n)              \n",
    "    Ua = np.random.randn(d, n)\n",
    "    va = np.random.randn(d)\n",
    "    context, alpha = additive_attention(s_prev, h, Wa, Ua, va)\n",
    "\n",
    "    print(\"Context vector (c_i):\", context)\n",
    "    print(\"Attention weights (α):\", alpha)\n",
    "    print(\"Sum of attention weights:\", np.sum(alpha))  # 应该接近 1\n",
    "    \n",
    "test_additive_attention()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-19T08:49:43.215728700Z",
     "start_time": "2025-04-19T08:49:43.206745600Z"
    }
   },
   "id": "279d3f27b2c6709a",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "再附赠一个pytorch版本的\n",
    "这里$W_a$和$U_a$用了linear实现，支持batch操作\n",
    "$$\n",
    "s_{exp} \\in \\mathbb{R}^{batch \\times T_x \\times hidden} \\quad \\@ \\quad W_a \\in \\mathbb{R}^{hidden \\times attn}\\quad \\Rightarrow \\quad \\mathbb{R}^{batch \\times T_x \\times attn}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f66a6e7e592cfefa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_prev shape: torch.Size([2, 4])\n",
      "h shape: torch.Size([2, 5, 4])\n",
      "context shape: torch.Size([2, 4])\n",
      "alpha shape: torch.Size([2, 5])\n",
      "alpha sum (per batch): tensor([1., 1.], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_dim):\n",
    "        super().__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, attention_dim, bias=False)\n",
    "        self.Ua = nn.Linear(hidden_size, attention_dim, bias=False)\n",
    "        self.va = nn.Parameter(torch.randn(attention_dim))  # learnable vector\n",
    "\n",
    "    def forward(self, s_prev:torch.Tensor, h):\n",
    "        \"\"\"\n",
    "        :param s_prev: (batch, hidden_size) decoder 上一步 hidden state\n",
    "        :param h: (batch, Tx, hidden_size) 所有 encoder hidden state\n",
    "        :return: context: (batch, hidden_size), alpha: (batch, Tx)\n",
    "        \"\"\"\n",
    "        # 1. expand s_prev → (batch, Tx, hidden_size)\n",
    "        batch_size, Tx, _ = h.size()\n",
    "        s_exp = s_prev.unsqueeze(1).expand(-1, Tx, -1)\n",
    "        # s_prev (batch, hidden_size) -> unsqueeze(1) (batch, 1, hidden_size) -> expand (batch, Tx, hidden_size)\n",
    "        # 所以s_exp =>(batch, Tx, hidden_size)\n",
    "        # 2. 计算注意力 energy scores\n",
    "        e = torch.tanh(self.Wa(s_exp) + self.Ua(h))  # (batch, Tx, attn_dim)\n",
    "        e_scores = torch.matmul(e, self.va)  # (batch, Tx) (batch, Tx, attn_dim) @ (attn_dim,) → (batch, Tx)\n",
    "\n",
    "        # 3. softmax 得到注意力分布\n",
    "        alpha = F.softmax(e_scores, dim=1)  # (batch, Tx)\n",
    "\n",
    "        # 4. 上下文向量 c_i = alpha @ h\n",
    "        context = torch.bmm(alpha.unsqueeze(1), h)  # torch.bmm((batch, 1, Tx), (batch, Tx, hidden)) → (batch, 1, hidden)\n",
    "        context = context.squeeze(1)  # (batch, hidden_size)\n",
    "\n",
    "        return context, alpha\n",
    "def test_torch_additive_attention():\n",
    "    batch_size = 2\n",
    "    Tx = 5              # encoder 时间步\n",
    "    hidden_size = 4     # encoder/decoder hidden dim\n",
    "    attention_dim = 3   # attention 隐空间维度\n",
    "\n",
    "    # 设置设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 随机输入：decoder state + encoder states\n",
    "    s_prev = torch.randn(batch_size, hidden_size).to(device)       # (batch, hidden_size)\n",
    "    h = torch.randn(batch_size, Tx, hidden_size).to(device)        # (batch, Tx, hidden_size)\n",
    "\n",
    "    # 初始化 attention 模块\n",
    "    attn = AdditiveAttention(hidden_size, attention_dim).to(device)\n",
    "\n",
    "    # 前向计算\n",
    "    context, alpha = attn(s_prev, h)\n",
    "\n",
    "    # 输出检查\n",
    "    print(\"s_prev shape:\", s_prev.shape)\n",
    "    print(\"h shape:\", h.shape)\n",
    "    print(\"context shape:\", context.shape)  # (batch, hidden_size)\n",
    "    print(\"alpha shape:\", alpha.shape)      # (batch, Tx)\n",
    "    print(\"alpha sum (per batch):\", alpha.sum(dim=1))  # 应该接近 1\n",
    "test_torch_additive_attention()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-19T08:55:32.719685Z",
     "start_time": "2025-04-19T08:55:32.195170500Z"
    }
   },
   "id": "96b5b04f8df91a2f",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%reset -f"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-19T08:57:40.377411100Z",
     "start_time": "2025-04-19T08:57:40.297194400Z"
    }
   },
   "id": "28e4776b3f151bfe",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Transformer: MHA Multi Head Attention\n",
    "\n",
    "如何从AA迁移到MHA:  $q = W_q s_{i-1} \\quad k_j = W_k h_j \\quad v_j = W_v h_j$\n",
    "这样看QKV就很类似了"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34ff8027a1d25299"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e15c549930f5949b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
